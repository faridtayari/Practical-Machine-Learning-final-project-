---
title: "Final Project Part I"
author: "FTayari"
date: "March 11, 2018"
output: html_document
---

In this report I used the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to  build models for making predictions on the train set and then appling the model to the test set for predicting the manner that participants did the exercise. 

Note: I experiences memory usage error. so, I needed to split the mark down file into two sections:
- Part 1: Tree, Pruning the tree with cross validation, and bagging
- part 2:Random forests, Boosting, LDA, QDA, 

```{r}
library(caret)
library(party)
library (tree)

training<-read.csv("pml-training.csv")
testing<-read.csv("pml-testing.csv")
```

#Cleaning the data and removing the unnecessary data
Droppng the summary statistics variables that include: 
"kurtosis_", "skewness_", "max_", "min_", "amplitude_", "var_", "avg_", and "stddev_" 

```{r}
training<-training[,c(8:11,37:49,60:68,84:86,116:124,151:160)]
testing<-testing[,c(8:11,37:49,60:68,84:86,116:124,151:160)]
sum(is.na(training))
sum(is.na(testing))
```

#Tree
```{r}
mod.tree <- train(classe ~ .,method="rpart",data=training)


#Train set prediction
pred.train<-predict(mod.tree, training)
train.table<-table(predict=pred.train,truth = training$classe )
train.table
(train.table[1,1]+train.table[2,2]+train.table[3,3]+train.table[4,4]+train.table[5,5])/sum(train.table)

# Test set prediction
pred.test<-predict(mod.tree, testing)
pred.test
```

As result show tree is not very accurate in train set prediction.

# Pruning the tree with cross validation
```{r}
mod.tree2 <- tree(classe ~ .,data=training)
mod.tree.cv <- cv.tree(mod.tree2 ,FUN=prune.misclass )
mod.tree.cv$size
par(mfrow =c(1,2))
plot(mod.tree.cv$size ,mod.tree.cv$dev ,type="b")
plot(mod.tree.cv$k ,mod.tree.cv$dev ,type="b")

mod.tree.prune =prune.misclass (mod.tree2, best =18)

#Train set prediction
pred.train<-predict(mod.tree.prune, training,type="class")
train.table<-table(predict=pred.train,truth = training$classe )
train.table
(train.table[1,1]+train.table[2,2]+train.table[3,3]+train.table[4,4]+train.table[5,5])/sum(train.table)

# Test set prediction
pred.test<-predict(mod.tree.prune, testing, type="class")
pred.test

```

Cross validation indicates that a tree with size 18 would be the best.
As results show pruned the tree does a better job in predicting the train set.

#bagging
```{r}
predictors = training[-c(dim(training)[2])]
classe = training$classe
treebag <- bag(predictors, classe, B = 10,
               bagControl = bagControl(fit = ctreeBag$fit,predict 
                                       = ctreeBag$pred, aggregate 
                                       = ctreeBag$aggregate))

# Train set prediction
pred.train<-predict(treebag,predictors)
train.table<-table(predict=pred.train,truth = training$classe )
train.table
(train.table[1,1]+train.table[2,2]+train.table[3,3]
        +train.table[4,4]+train.table[5,5])/sum(train.table)

# Test set prediction
predictors.test<-testing[-c(dim(testing)[2])]
predictors.test[,35]<-as.numeric(predictors.test[,35])
predictors.test[,46]<-as.numeric(predictors.test[,46])
predictors.test[,47]<-as.numeric(predictors.test[,47])
pred.test<-predict(treebag,predictors.test)
pred.test

```

As result show, it seems bagging is working very well on the train set.